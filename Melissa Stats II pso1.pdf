\documentclass[12pt,letterpaper]{article}
\usepackage{graphicx,textcomp}
\usepackage{natbib}
\usepackage{setspace}
\usepackage{fullpage}
\usepackage{color}
\usepackage[reqno]{amsmath}
\usepackage{amsthm}
\usepackage{fancyvrb}
\usepackage{amssymb,enumerate}
\usepackage[all]{xy}
\usepackage{endnotes}
\usepackage{lscape}
\newtheorem{com}{Comment}
\usepackage{float}
\usepackage{hyperref}
\newtheorem{lem} {Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{thm}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{cor}{Corollary}
\newtheorem{obs}{Observation}
\usepackage[compact]{titlesec}
\usepackage{dcolumn}
\usepackage{tikz}
\usetikzlibrary{arrows}
\usepackage{multirow}
\usepackage{xcolor}
\newcolumntype{.}{D{.}{.}{-1}}
\newcolumntype{d}[1]{D{.}{.}{#1}}
\definecolor{light-gray}{gray}{0.65}
\usepackage{url}
\usepackage{listings}
\usepackage{color}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}
\lstset{style=mystyle}
\newcommand{\Sref}[1]{Section~\ref{#1}}
\newtheorem{hyp}{Hypothesis}

\title{Problem Set 1}
\date{Due: February 12, 2023}
\author{Applied Stats II - Melissa Campbell}


\begin{document}
	\maketitle
	\section*{Instructions}
	\begin{itemize}
		\item Please show your work! You may lose points by simply writing in the answer. If the problem requires you to execute commands in \texttt{R}, please include the code you used to get your answers. Please also include the \texttt{.R} file that contains your code. If you are not sure if work needs to be shown for a particular problem, please ask.
		\item Your homework should be submitted electronically on GitHub in \texttt{.pdf} form.
		\item This problem set is due before 23:59 on Sunday February 12, 2023. No late assignments will be accepted.
	\end{itemize}
	
	\vspace{.25cm}
	\section*{Question 1} 
	\vspace{.25cm}
	\noindent The Kolmogorov-Smirnov test uses cumulative distribution statistics test the similarity of the empirical distribution of some observed data and a specified PDF, and serves as a goodness of fit test. The test statistic is created by:
	
	$$D = \max_{i=1:n} \Big\{ \frac{i}{n}  - F_{(i)}, F_{(i)} - \frac{i-1}{n} \Big\}$$
	
	\noindent where $F$ is the theoretical cumulative distribution of the distribution being tested and $F_{(i)}$ is the $i$th ordered value. Intuitively, the statistic takes the largest absolute difference between the two distribution functions across all $x$ values. Large values indicate dissimilarity and the rejection of the hypothesis that the empirical distribution matches the queried theoretical distribution. The p-value is calculated from the Kolmogorov-
	Smirnoff CDF:
	
	$$p(D \leq x) \frac{\sqrt {2\pi}}{x} \sum _{k=1}^{\infty }e^{-(2k-1)^{2}\pi ^{2}/(8x^{2})}$$
	
	
	\noindent which generally requires approximation methods (see \href{https://core.ac.uk/download/pdf/25787785.pdf}{Marsaglia, Tsang, and Wang 2003}). This so-called non-parametric test (this label comes from the fact that the distribution of the test statistic does not depend on the distribution of the data being tested) performs poorly in small samples, but works well in a simulation environment. Write an \texttt{R} function that implements this test where the reference distribution is normal. Using \texttt{R} generate 1,000 Cauchy random variables (\texttt{rcauchy(1000, location = 0, scale = 1)}) and perform the test (remember, use the same seed, something like \texttt{set.seed(123)}, whenever you're generating your own data).\\
	
	
	\noindent As a hint, you can create the empirical distribution and theoretical CDF using this code:
	
	\begin{lstlisting}[language=R]
		# create empirical distribution of observed data
		ECDF <- ecdf(data)
		empiricalCDF <- ECDF(data)
		# generate test statistic
		D <- max(abs(empiricalCDF - pnorm(data))) 
	
	library(dgof)
	library(tidyverse) 
	library(ggpubr)
	> library(tidyverse) 
	> library(ggpubr)
	> # set wd for current folder
	> setwd("C:/Users/le_ba/Documents/GitHub/StatsI_Fall2022/problemSets/PS01")
	> set.seed(123)
	> # create empirical distribution of observed data
	> ECDF <- ecdf(data)
	Error in sort.int(x, na.last = na.last, decreasing = decreasing, ...) : 
	'x' must be atomic
	> empiricalCDF <- ECDF(data)
	Error in ECDF(data) : could not find function "ECDF"
	> # generate test statistic
	> D <- max(abs(empiricalCDF - pnorm(data)))
	Error: object 'empiricalCDF' not found
	> # sample 1
	> # generating a random sample from a normal distribution
	> x <- rnorm(1000, mean=0, sd=4)
	> # Create a histogram to visualise sample 1
	> hist(x,                                       
	+      xlim = c(- 10, 10),
	+      breaks = 100,
	+      main = "Visualising Sample 1")
	> # sample 2
	> # Generate 1000 observations in Cauchy distribution
	> x2 <- rcauchy(1000, location = -1, scale = 10)
	> # Create a histogram to visualise sample 2
	> hist(x2,                                       
	+      xlim = c(- 200, 200),
	+      breaks = 10000,
	+      main = "Visualising Sample 2")
	> # apply the ecdf function in order to calculate the ECDF values of data
	> ecdf(x)
	Empirical CDF 
	Call: ecdf(x)
	x[1:1000] = -11.239, -10.644, -10.573,  ..., 10.767, 12.964
	> ecdf(x2)
	Empirical CDF 
	Call: ecdf(x2)
	x[1:1000] = -10442,  -7104, -969.94,  ..., 5001.2,  20742
	> # plotting the results
	> plot(ecdf(x),
	+      col = "green")
	> plot(ecdf(x),
	+      xlim = range(c(x, x2)),
	+      col = "blue")
	> plot(ecdf(x2),
	+      add = TRUE,
	+      lty = "dashed",
	+      col = "red")
	> #find mean of sample
	> mean(data)
	[1] NA
	Warning message:
	In mean.default(data) : argument is not numeric or logical: returning NA
	> #find standard deviation of sample
	> sd(data)
	Error in as.double(x) : 
	cannot coerce type 'closure' to vector of type 'double'
	> # calculate standard error
	> print(sqrt(sum((x - mean(x)) ^ 2/(length(x) - 1)))
	+       /sqrt(length(x)))
	[1] 0.1254406
	> #Perform t-test to find the p-value
	> t.test(x)
	
	One Sample t-test
	
	data:  x
	t = 0.51428, df = 999, p-value = 0.6072
	alternative hypothesis: true mean is not equal to 0
	95 percent confidence interval:
	-0.1816458  0.3106687
	sample estimates:
	mean of x 
	0.06451146 
	
	> # run Kolmogorov-Smirnov test
	> ks.test(x, "pnorm")
	
	One-sample Kolmogorov-Smirnov test
	
	data:  x
	D = 0.28598, p-value < 2.2e-16
	alternative hypothesis: two-sided
	
	> ecdf(x2)
	Empirical CDF 
	Call: ecdf(x2)
	x[1:1000] = -10442,  -7104, -969.94,  ..., 5001.2,  20742
	> max(x2, na.rm = FALSE)
	[1] 20741.51
	> min(x2, na.rm = FALSE)
	[1] -10442.08
	> # run Kolmogorov-Smirnov test just on x2
	> ks.test(x2, "pnorm")
	
	One-sample Kolmogorov-Smirnov test
	
	data:  x2
	D = 0.44625, p-value < 2.2e-16
	alternative hypothesis: two-sided
	
	> # #perform Kolmogorov-Smirnov test on both x and x2
	> ks.test(x, x2)
	
	Two-sample Kolmogorov-Smirnov test
	
	data:  x and x2
	D = 0.289, p-value < 2.2e-16
	alternative hypothesis: two-sided \end{lstlisting}
	\vspace{3in}
	
	\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{Rplot01}
	\caption{}
	\label{fig:rplot01}
	\end{figure}
	
	\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{Rplot2}
	\caption{}
	\label{fig:rplot2}
	\end{figure}
	
	
	\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{Rplot3}
	\caption{}
	\label{fig:rplot3}
	\end{figure}
	
	\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{Rplot4}
	\caption{}
	\label{fig:rplot4}
	\end{figure}
	
	\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{Rplot5}
	\caption{}
	\label{fig:rplot5}
	\end{figure}
	
	
	
	
	\section*{Question 2}
	\noindent Estimate an OLS regression in \texttt{R} that uses the Newton-Raphson algorithm (specifically \texttt{BFGS}, which is a quasi-Newton method), and show that you get the equivalent results to using \texttt{lm}. Use the code below to create your data.
	\vspace{.5cm}
	

	\begin{lstlisting}[language=R]
	> set.seed (123)
	> data <- data.frame(x = runif(200, 1, 10))
	> data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5)
	> # Preparing the Data and Design Matrix
	> Y <- matrix(data$y, nrow = nrow(data), ncol = ncol(data))
	> n <- ncol(data) 
	> optim(n, 200, gr = NULL, 
	+       method = c("BFGS"),
	+       lower = -Inf, upper = Inf,
	+       control = list(), hessian = FALSE)
	
	> model <- lm(y ~ x, data=data)
	> model
	
	Call:
	lm(formula = y ~ x, data = data)
	
	Coefficients:
	(Intercept)            x  
	0.1392       2.7267  \end{lstlisting}
\end{document}